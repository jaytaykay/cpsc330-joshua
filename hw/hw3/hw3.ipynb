{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 3: Preprocessing \n",
    "### Associated lectures: [Lectures 4, 5, 6](https://github.com/UBC-CS/cpsc330-2023s/tree/main/lectures) \n",
    "\n",
    "**Due date: Monday, Feb 5, 11:59pm. See the [HW schedule](https://github.com/UBC-CS/cpsc330-2023W2#deliverable-due-dates-tentative)**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "<hr>\n",
    "\n",
    "_Points: 6_\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2023W2/blob/master/docs/homework_instructions.md), and the summary at the end of this HW.\n",
    "\n",
    "**You <u>may</u> work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- You can also work on your own if you prefer.\n",
    "- The maximum group size is 2.\n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "\n",
    "_Note: The assignments will get gradually more open-ended as we progress through the course. In many cases, there won't be a single correct solution. Sometimes you will have to make your own choices and your own decisions (for example, on what parameter values to use when they are not explicitly provided in the instructions). Use your own judgment in such cases and justify your choices, if necessary._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3247a4b883a670c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "A crucial step when using machine learning algorithms on real-world datasets is preprocessing. This homework will give you some practice of data preprocessing and building a supervised machine learning pipeline on a real-world dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Introducing the dataset\n",
    "<hr>\n",
    "\n",
    "In this lab, you will be working on [the adult census dataset](https://www.kaggle.com/uciml/adult-census-income#). Download the CSV and save it as `adult.csv` under the data folder in this homework folder. \n",
    "\n",
    "This is a classification dataset and the classification task is to predict whether income exceeds 50K per year or not based on the census data. You can find more information on the dataset and features [here](http://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "\n",
    "The starter code below loads the data CSV (assuming that it is saved as `adult.csv` under the data folder). \n",
    "\n",
    "_Note that many popular datasets have sex as a feature where the possible values are male and female. This representation reflects how the data were collected and is not meant to imply that, for example, gender is binary._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_df = pd.read_csv(\"data/adult.csv\")\n",
    "census_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data splitting \n",
    "\n",
    "_Points: 2_\n",
    "\n",
    "In order to avoid violation of the golden rule, the first step before we do anything is splitting the data. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into `train_df` (40%) and `test_df` (60%) with `random_state = 123`. Keep the target column (`income`) in the splits so that we can use it in the exploratory data analysis.  \n",
    "\n",
    "_Usually having more data for training is a good idea. But here I'm using 40%/60% split because running cross-validation with this dataset can take a while on a modest laptop. A smaller training data means it won't take too long to train the model on your laptop. A side advantage of this would be that with a bigger test split, we'll have a more reliable estimate of the model performance!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(census_df, train_size=0.4, test_size=0.6, random_state=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2: Exploratory data analysis (EDA) <a name=\"2\"></a> \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine our `train_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>?</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>216864</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>150601</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32554</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>116138</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>310152</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13024 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age workclass  fnlwgt     education  education.num      marital.status  \\\n",
       "0       90         ?   77053       HS-grad              9             Widowed   \n",
       "1       82   Private  132870       HS-grad              9             Widowed   \n",
       "4       41   Private  264663  Some-college             10           Separated   \n",
       "5       34   Private  216864       HS-grad              9            Divorced   \n",
       "6       38   Private  150601          10th              6           Separated   \n",
       "...    ...       ...     ...           ...            ...                 ...   \n",
       "32554   32   Private  116138       Masters             14       Never-married   \n",
       "32556   22   Private  310152  Some-college             10       Never-married   \n",
       "32557   27   Private  257302    Assoc-acdm             12  Married-civ-spouse   \n",
       "32559   58   Private  151910       HS-grad              9             Widowed   \n",
       "32560   22   Private  201490       HS-grad              9       Never-married   \n",
       "\n",
       "            occupation   relationship                race     sex  \\\n",
       "0                    ?  Not-in-family               White  Female   \n",
       "1      Exec-managerial  Not-in-family               White  Female   \n",
       "4       Prof-specialty      Own-child               White  Female   \n",
       "5        Other-service      Unmarried               White  Female   \n",
       "6         Adm-clerical      Unmarried               White    Male   \n",
       "...                ...            ...                 ...     ...   \n",
       "32554     Tech-support  Not-in-family  Asian-Pac-Islander    Male   \n",
       "32556  Protective-serv  Not-in-family               White    Male   \n",
       "32557     Tech-support           Wife               White  Female   \n",
       "32559     Adm-clerical      Unmarried               White  Female   \n",
       "32560     Adm-clerical      Own-child               White    Male   \n",
       "\n",
       "       capital.gain  capital.loss  hours.per.week native.country income  \n",
       "0                 0          4356              40  United-States  <=50K  \n",
       "1                 0          4356              18  United-States  <=50K  \n",
       "4                 0          3900              40  United-States  <=50K  \n",
       "5                 0          3770              45  United-States  <=50K  \n",
       "6                 0          3770              40  United-States  <=50K  \n",
       "...             ...           ...             ...            ...    ...  \n",
       "32554             0             0              11         Taiwan  <=50K  \n",
       "32556             0             0              40  United-States  <=50K  \n",
       "32557             0             0              38  United-States  <=50K  \n",
       "32559             0             0              40  United-States  <=50K  \n",
       "32560             0             0              20  United-States  <=50K  \n",
       "\n",
       "[13024 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some missing values represented with a \"?\". Probably these were the questions not answered by some people during the census.  Usually `.describe()` or `.info()` methods would give you information on missing values. But here, they won't pick \"?\" as missing values because they are encoded as strings instead of an actual NaN in Python. So let's replace them with `np.nan` before we carry out EDA. If you do not do it, you'll encounter an error later on when you try to pass this data to a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13024, 15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.replace(\"?\", np.nan)\n",
    "test_df = test_df.replace(\"?\", np.nan)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>216864</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>150601</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32554</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>116138</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>310152</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13024 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age workclass  fnlwgt     education  education.num      marital.status  \\\n",
       "0       90       NaN   77053       HS-grad              9             Widowed   \n",
       "1       82   Private  132870       HS-grad              9             Widowed   \n",
       "4       41   Private  264663  Some-college             10           Separated   \n",
       "5       34   Private  216864       HS-grad              9            Divorced   \n",
       "6       38   Private  150601          10th              6           Separated   \n",
       "...    ...       ...     ...           ...            ...                 ...   \n",
       "32554   32   Private  116138       Masters             14       Never-married   \n",
       "32556   22   Private  310152  Some-college             10       Never-married   \n",
       "32557   27   Private  257302    Assoc-acdm             12  Married-civ-spouse   \n",
       "32559   58   Private  151910       HS-grad              9             Widowed   \n",
       "32560   22   Private  201490       HS-grad              9       Never-married   \n",
       "\n",
       "            occupation   relationship                race     sex  \\\n",
       "0                  NaN  Not-in-family               White  Female   \n",
       "1      Exec-managerial  Not-in-family               White  Female   \n",
       "4       Prof-specialty      Own-child               White  Female   \n",
       "5        Other-service      Unmarried               White  Female   \n",
       "6         Adm-clerical      Unmarried               White    Male   \n",
       "...                ...            ...                 ...     ...   \n",
       "32554     Tech-support  Not-in-family  Asian-Pac-Islander    Male   \n",
       "32556  Protective-serv  Not-in-family               White    Male   \n",
       "32557     Tech-support           Wife               White  Female   \n",
       "32559     Adm-clerical      Unmarried               White  Female   \n",
       "32560     Adm-clerical      Own-child               White    Male   \n",
       "\n",
       "       capital.gain  capital.loss  hours.per.week native.country income  \n",
       "0                 0          4356              40  United-States  <=50K  \n",
       "1                 0          4356              18  United-States  <=50K  \n",
       "4                 0          3900              40  United-States  <=50K  \n",
       "5                 0          3770              45  United-States  <=50K  \n",
       "6                 0          3770              40  United-States  <=50K  \n",
       "...             ...           ...             ...            ...    ...  \n",
       "32554             0             0              11         Taiwan  <=50K  \n",
       "32556             0             0              40  United-States  <=50K  \n",
       "32557             0             0              38  United-States  <=50K  \n",
       "32559             0             0              40  United-States  <=50K  \n",
       "32560             0             0              20  United-States  <=50K  \n",
       "\n",
       "[13024 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"?\" symbols are now replaced with NaN values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 `describe()` method\n",
    "\n",
    "_Points: 5_\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Examine the output of `train_df.describe()` with `include='all'` argument and store it in a variable called `census_summary`.\n",
    "2. What is the highest hours per week someone reported? Store it in a variable called `max_hours_per_week`.\n",
    "3. What is the most frequently occurring occupation in this dataset? Store it in a variable called `most_freq_occupation`.\n",
    "4. Store the column names of the columns with missing values as a list in a variable called `missing_vals_cols`. \n",
    "5. Store the column names of all numeric-looking columns as a list in a variable called `numeric_cols`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13024.000000</td>\n",
       "      <td>12284</td>\n",
       "      <td>1.302400e+04</td>\n",
       "      <td>13024</td>\n",
       "      <td>13024.000000</td>\n",
       "      <td>13024</td>\n",
       "      <td>12281</td>\n",
       "      <td>13024</td>\n",
       "      <td>13024</td>\n",
       "      <td>13024</td>\n",
       "      <td>13024.000000</td>\n",
       "      <td>13024.000000</td>\n",
       "      <td>13024.000000</td>\n",
       "      <td>12783</td>\n",
       "      <td>13024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5936</td>\n",
       "      <td>1649</td>\n",
       "      <td>5195</td>\n",
       "      <td>11158</td>\n",
       "      <td>8700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11717</td>\n",
       "      <td>9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.546913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.884759e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.057432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1079.831695</td>\n",
       "      <td>86.543074</td>\n",
       "      <td>40.395654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.610225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.040895e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.553084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7322.034546</td>\n",
       "      <td>403.025863</td>\n",
       "      <td>12.285347</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.376900e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.170962e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.778990e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.365655e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.184622e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age workclass        fnlwgt education  education.num  \\\n",
       "count   13024.000000     12284  1.302400e+04     13024   13024.000000   \n",
       "unique           NaN         8           NaN        16            NaN   \n",
       "top              NaN   Private           NaN   HS-grad            NaN   \n",
       "freq             NaN      9123           NaN      4153            NaN   \n",
       "mean       38.546913       NaN  1.884759e+05       NaN      10.057432   \n",
       "std        13.610225       NaN  1.040895e+05       NaN       2.553084   \n",
       "min        17.000000       NaN  1.376900e+04       NaN       1.000000   \n",
       "25%        28.000000       NaN  1.170962e+05       NaN       9.000000   \n",
       "50%        37.000000       NaN  1.778990e+05       NaN      10.000000   \n",
       "75%        48.000000       NaN  2.365655e+05       NaN      12.000000   \n",
       "max        90.000000       NaN  1.184622e+06       NaN      16.000000   \n",
       "\n",
       "            marital.status      occupation relationship   race    sex  \\\n",
       "count                13024           12281        13024  13024  13024   \n",
       "unique                   7              14            6      5      2   \n",
       "top     Married-civ-spouse  Prof-specialty      Husband  White   Male   \n",
       "freq                  5936            1649         5195  11158   8700   \n",
       "mean                   NaN             NaN          NaN    NaN    NaN   \n",
       "std                    NaN             NaN          NaN    NaN    NaN   \n",
       "min                    NaN             NaN          NaN    NaN    NaN   \n",
       "25%                    NaN             NaN          NaN    NaN    NaN   \n",
       "50%                    NaN             NaN          NaN    NaN    NaN   \n",
       "75%                    NaN             NaN          NaN    NaN    NaN   \n",
       "max                    NaN             NaN          NaN    NaN    NaN   \n",
       "\n",
       "        capital.gain  capital.loss  hours.per.week native.country income  \n",
       "count   13024.000000  13024.000000    13024.000000          12783  13024  \n",
       "unique           NaN           NaN             NaN             40      2  \n",
       "top              NaN           NaN             NaN  United-States  <=50K  \n",
       "freq             NaN           NaN             NaN          11717   9875  \n",
       "mean     1079.831695     86.543074       40.395654            NaN    NaN  \n",
       "std      7322.034546    403.025863       12.285347            NaN    NaN  \n",
       "min         0.000000      0.000000        1.000000            NaN    NaN  \n",
       "25%         0.000000      0.000000       40.000000            NaN    NaN  \n",
       "50%         0.000000      0.000000       40.000000            NaN    NaN  \n",
       "75%         0.000000      0.000000       45.000000            NaN    NaN  \n",
       "max     99999.000000   4356.000000       99.000000            NaN    NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_summary = train_df.describe(include='all')\n",
    "census_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_hours_per_week = train_df['hours.per.week'].max()\n",
    "max_hours_per_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Prof-specialty\n",
       "Name: occupation, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq_occupation = train_df['occupation'].mode()\n",
    "most_freq_occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'fnlwgt',\n",
       " 'education.num',\n",
       " 'capital.gain',\n",
       " 'capital.loss',\n",
       " 'hours.per.week']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_has_null = train_df.isnull().any()\n",
    "missing_vals_cols = col_has_null[col_has_null==True].index.tolist()\n",
    "missing_vals_cols\n",
    "numeric_cols = train_df.select_dtypes(include='number').columns.tolist()\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Visualizing features\n",
    "\n",
    "_Points: 5_\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Visualize the histograms of numeric features.  \n",
    "2. From the visualizations, which features seem relevant for the given prediction task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Identify transformations to apply\n",
    "\n",
    "_Points: 13_\n",
    "\n",
    "Before passing this data to a machine learning model, we need to apply some transformations on different features. Below we are providing possible transformations which can be applied on each column in `census_df`.  \n",
    "\n",
    "**Your tasks:**\n",
    "1. Write your justification or explanation for each row in the explanation column. An example explanation is given for the age column. \n",
    "\n",
    "> Note: This question is a bit open-ended. If you do not agree with the provided transformation, feel free to argue your case in the explanation. That said, in this assignment, go with the transformations provided below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Transformation | Explanation\n",
    "| --- | ----------- | ----- |\n",
    "| age | scaling |  A numeric feature with no missing values. Good idea to apply scaling, as the range of values (17 to 90) is quite different compared to other numeric features.|\n",
    "| workclass | imputation, one-hot encoding | |\n",
    "| fnlwgt | drop |  |\n",
    "| education | ordinal encoding | |\n",
    "| education.num | drop | |\n",
    "| marital.status | one-hot encoding  | |\n",
    "| occupation | imputation, one-hot encoding  | |\n",
    "| relationship | one-hot encoding  | |\n",
    "| race | drop  |  |\n",
    "| sex | one-hot encoding with \"binary=True\" | |\n",
    "| capital.gain | scaling |  | \n",
    "| capital.loss | scaling |  |\n",
    "| hours.per.week | scaling | |\n",
    "| native.country | imputation, one-hot encoding | | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Identify feature types \n",
    "\n",
    "_Points: 5_\n",
    "\n",
    "**Your tasks:**\n",
    "1. Based on the types of transformations you want to apply on the features, identify different feature types and store them in the variables below as lists.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_2.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Fill in the lists below.\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "ordinal_features = []\n",
    "binary_features = []\n",
    "drop_features = []\n",
    "target = \"income\"\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Baseline models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Separating feature vectors and targets  \n",
    "\n",
    "_Points: 2_\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create `X_train`, `y_train`, `X_test`, `y_test` from `train_df` and `test_df`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = None\n",
    "y_train = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dummy classifier\n",
    "\n",
    "_Points: 2_\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Carry out 5-fold cross-validation using `scikit-learn`'s `cross_validate` function with `return_train_scores=True` and store the results as a dataframe named `dummy_df` where each row corresponds to the results from a cross-validation fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "dummy_df = None \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.3 Discussion\n",
    "\n",
    "_Points: 2_\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Hopefully, you were able to run cross-validation with dummy classifier successfully in the question above. At this point, if you train [`sklearn`'s `SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on `X_train` and `y_train` would it work? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Preprocessing\n",
    "<hr>\n",
    "\n",
    "In this dataset, we have different types of features: numeric features, an ordinal feature, categorical features, and a binary feature. We want to apply different transformations on different columns and therefore we need a column transformer. In this exercise, first, we'll define different transformations on different types of features and then will create a `scikit-learn`'s `ColumnTransformer`. For example, the code below creates a `numeric_transformer` for numeric features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercises below, you'll create transformers for other types of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preprocessing ordinal features\n",
    "\n",
    "_Points: 5_\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a transformer called `ordinal_transformer` for our ordinal features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "ordinal_transformer = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Preprocessing binary features\n",
    "\n",
    "_Points: 2_\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a transformer called `binary_transformer` for our binary features.\n",
    "\n",
    "> _Note that many popular datasets have sex as a feature where the possible values are male and female. This representation reflects how the data were collected and is not meant to imply that, for example, gender is binary._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "binary_transformer = None\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Preprocessing categorical features\n",
    "\n",
    "_Points: 4_\n",
    "\n",
    "In Exercise 2.3, we saw that there are 3 categorical features with missing values. So first we need to impute the missing values and then encode these features with one-hot encoding. For the purpose of this assignment, let's just have imputation as the first step for all categorical features even when they do not have missing values. This should be OK because if a feature doesn't have any missing value,  imputation won't be applied. \n",
    "\n",
    "If we want to apply more than one transformation on a set of features, we need to create a [`scikit-learn` `Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). For example, for categorical features we can create a `scikit-learn` `Pipeline` with first step as imputation and the second step as one-hot encoding. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a `sklearn` `Pipeline` using [`make_pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) called `categorical_transformer` for our categorical features with two steps: `SimpleImputer` for imputation with `strategy=\"constant\"` and `fill_value=\"missing\"` and `OneHotEncoder` with `handle_unknown=\"ignore\"` and `sparse=False` for one-hot encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "categorical_transformer = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Creating a column transformer. \n",
    "\n",
    "_Points: 7_\n",
    "\n",
    "**Your tasks:**\n",
    "1. Create a `sklearn` `ColumnTransformer` named `preprocessor` using [`make_column_transformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html) with the transformers defined in the previous exercises. Use the sequence below in the column transformer and add a \"drop\" step for the `drop_features` in the end.  \n",
    "    - `numeric_transformer`\n",
    "    - `ordinal_transformer`\n",
    "    - `binary_transformer`\n",
    "    - `categorical_transformer`\n",
    "2. Transform the data by calling `fit_transform` on the training set and save it as a dataframe in a variable called `transformed_df`. How many new columns have been created in the preprocessed data in comparison to the original `X_train`? Store the difference between the number of columns in `transformed_df` and `X_train` in a variable called `n_new_cols`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "preprocessor = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "transformed_df = None\n",
    "n_new_cols = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Building models \n",
    "\n",
    "Now that we have preprocessed features, we are ready to build models. Below, I'm providing the function we used in class which returns mean cross-validation score along with standard deviation for a given model. Use it to keep track of your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "results_dict = {}  # dictionary to store all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I'm showing an example where I call `mean_std_cross_val_scores` with `DummyClassifier`. The function calls `cross_validate` with the passed arguments and returns a series with mean cross-validation results and std of cross-validation. When you train new models, you can just add the results of these models in `results_dict`, which can be easily converted to a dataframe so that you can have a table with all your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(random_state = 42)\n",
    "pipe = make_pipeline(preprocessor, dummy)\n",
    "results_dict[\"dummy\"] = mean_std_cross_val_scores(\n",
    "    pipe, X_train, y_train, cv=5, return_train_score=True\n",
    ")\n",
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 5.1 Trying different classifiers\n",
    "\n",
    "_Points: 10_\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. For each of the models in the starter code below: \n",
    "    - Define a pipeline with two steps: `preprocessor` from 4.4 and the model as your classifier. \n",
    "    - Carry out 5-fold cross-validation with the pipeline and get the mean cross-validation scores with std by calling the `mean_std_cross_val_scores` function above. \n",
    "    - Store the results in a dataframe called `income_pred_results_df` with the model names in the `models` dictionary below as the index and each row representing results returned by `mean_std_cross_val_scores` function above. In other words, `income_pred_results_df` should look similar to the `results_df` dataframe above with more rows for the models below. \n",
    "    \n",
    "> This might take a while to run. Be patient! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"decision tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"kNN\": KNeighborsClassifier(),\n",
    "    \"RBF SVM\": SVC(random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "income_pred_results_df = None \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 5.2 Discussion \n",
    "\n",
    "_Points: 5_\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Examine the train and validation accuracies and `fit` and `score` times for all the models in the results above. How do the validation accuracies compare to the `DummyClassifier` model? Which model has the best validation accuracy? Which model is the fastest one? Which model is overfitting the most and the least?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 5.3 Hyperparameter optimization\n",
    "\n",
    "_Points: 10_\n",
    "\n",
    "In this exercise, you'll carry out hyperparameter optimization for the hyperparameter `C` of SVC RBF classifier. In practice, you'll carry out hyperparameter optimization for all different hyperparameters of the most promising classifiers. For the purpose of this assignment, we'll only do it for the `SVC` classifier with one hyperparameter, namely `C`. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. For each `C` value in the `param_grid` below: \n",
    "    - Create a pipeline object with two steps: preprocessor from 4.4 and `SVC` classifier with the `C` value.\n",
    "    - Carry out 5-fold cross validation with the pipeline.  \n",
    "    - Store the results in `results_dict` and display results as a pandas DataFrame. \n",
    "2. Which hyperparameter value seems to be performing the best? Store it in a variable called `best_C`. (Since this question is not autograded, please store the value directly as a number, something like `best_C = 0.001`, if `C = 0.001` is giving you the best CV score.) Is it different than the default value for the hyperparameter used by `scikit-learn`? \n",
    "\n",
    "> Note: Running this will take a while. Please be patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"C\": np.logspace(-1, 2, 4)}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "best_C = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Exercise 6: Evaluating on the test set \n",
    "<hr>\n",
    "\n",
    "Now that we have a best performing model, it's time to assess our model on the set aside test set. In this exercise, you'll examine whether the results you obtained using cross-validation on the train set are consistent with the results on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Scoring on the unseen test set \n",
    "\n",
    "_Points: 4_\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a pipeline named `final_pipeline` with the preprocessor from 4.4 as the first step and the best performing SVC model from 5.4 as the second step. \n",
    "2. Train the pipeline on the entire training set `X_train` and `y_train`. \n",
    "3. Score the pipeline on `X_test` and `y_test` and store the score in a variable called `test_score`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_6.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "final_pipeline = None\n",
    "test_score = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Short answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 7.1 kNNs toy example\n",
    "\n",
    "_Points: 3_\n",
    "\n",
    "Suppose you want to get predictions using the $k$-nearest neighbour algorithm on the toy dataset below. \n",
    "\n",
    "$$ X = \\begin{bmatrix}5 & 2\\\\4 & -2\\\\  2 & 2\\\\ 10 & 10\\\\ 9 & -1\\\\ 9& 9\\end{bmatrix}, \\quad y = \\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\1\\\\2\\end{bmatrix}.$$\n",
    "\n",
    "Suppose you are given a new data point $x=\\begin{bmatrix} 0\\\\0\\end{bmatrix}$.  \n",
    "\n",
    "1. What would the model predict when $k=1$?\n",
    "2. What would the model predict when $k=3$?\n",
    "3. What would the model predict when $k=3$ if we were doing regression rather than classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 7.2 Preprocessing\n",
    "\n",
    "_Points: 8_\n",
    "\n",
    "1. What is the problem with calling `fit_transform` on your test data with `StandardScaler`?\n",
    "2. Why is it important to follow the Golden Rule? If you violate it, will that give you a worse classifier?\n",
    "3. What are two advantages of using sklearn Pipelines? \n",
    "4. When is it appropriate to use sklearn `ColumnTransformer`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using [PrairieLearn](https://ca.prairielearn.com/pl/course_instance/6697)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing the homework! This was a tricky one but I hope you are feeling good after working on it. You are now ready to build a simple supervised machine learning pipeline on real-world datasets! Well done :clap:! \n",
    "\n",
    "![](img/eva-well-done.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not train_df is None and not test_df is None, \"Please use the provided variables.\"\n>>> assert train_df.shape == (13024, 15), \"The dimensions of the training set are incorrect\"\n>>> assert test_df.shape == (19537, 15), \"The dimensions of the test set are incorrect\"\n>>> assert train_df.loc[12846][['age', 'education', 'occupation', 'capital.loss']].tolist() == [49, 'Some-college', 'Craft-repair', 0], \"Are you using the provided random state?\"\n>>> assert not 20713 in train_df.index, 'Are you using the provided random state?' \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.1": {
     "name": "q2.1",
     "points": [
      1,
      1,
      1,
      1,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # Task 1\n>>> assert isinstance(census_summary, pd.DataFrame), \"census_summary dataftame is not created\"\n>>> assert census_summary.shape == (11, 15), \"census_summary shape is incorrect. Probably you are not including all columns\"\n>>> assert census_summary.loc['min']['age'] == 17.0, \"census_summary dataframe is incorrect\"\n>>> assert census_summary.loc['top']['occupation'] == \"Prof-specialty\", \"census_summary dataframe is incorrect\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 2\n>>> assert (sha1(str(max_hours_per_week).encode('utf8')).hexdigest() == \"3359de52c8ae993fe0f8fe9c5168a0065bd3c7a4\"), \"max_hours_per_week are incorrect\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 3\n>>> assert (sha1(str(most_freq_occupation).encode('utf8')).hexdigest() == \"97165f50eddb0d28a382b0366274e2fe38505644\"), \"most_freq_occupation is incorrect\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 4\n>>> assert (sha1(str(missing_vals_cols).encode('utf8')).hexdigest() == \"6bc5e13d4d66b306e52701ee9a1e5e21bf19aeb0\"), \"Please use the exact column/feature name. Also, make sure the lists are sorted.\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 5\n>>> assert (sha1(str(numeric_cols).encode('utf8')).hexdigest() == \"615afaf5011128d641ab8a73289d57bd01a3ec37\"), \"Please use the exact column/feature name. Also, make sure the lists are sorted.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.4": {
     "name": "q2.4",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert (sha1(str(numeric_features).encode('utf8')).hexdigest() == \"71401cf60034fd69eee7398866359f612adf3e15\"), \"numeric_features list is not correct\"\n>>> assert (sha1(str(categorical_features).encode('utf8')).hexdigest() == \"af1a4022c0362405678be5c3a6735578a8c0069f\"), \"categorical_features list is not correct\"\n>>> assert (sha1(str(ordinal_features).encode('utf8')).hexdigest() == \"95b86602c44211f3ad662bb58b8e53d024106d05\"), \"ordinal_features list is not correct\"\n>>> assert (sha1(str(binary_features).encode('utf8')).hexdigest() == \"d4b7aa4c56ac2f98e6ac9cec7768484b415b7337\"), \"binary_features list is not correct\"\n>>> assert (sha1(str(drop_features).encode('utf8')).hexdigest() == \"62aab57d42c54be3dfd3c55020e5a167ca1a84c3\"), \"drop_features list is not correct\"\n>>> assert (sha1(str(target).encode('utf8')).hexdigest() == \"0f613350b66e64d92ef21bc4dcdbf8996cb4edf0\"), \"target variable is not set correctly\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not X_train is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert not y_train is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert not X_test is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert not y_test is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert X_train.shape == (13024, 14), \"The dimensions of X_train are incorrect\"\n>>> assert y_train.shape == (13024, ), \"The dimensions of y_train are incorrect. Are you splitting correctly\"\n>>> assert X_test.shape == (19537,14), \"The dimensions of X_test are incorrect. Are you splitting correctly? Are you using single brackets?\"\n>>> assert y_test.shape == (19537,), \"The dimensions of y_test are incorrect. Are you splitting correctly? Are you using single brackets?\"\n>>> assert 'income' not in list(X_train.columns), \"Make sure the target variable is not part of your X dataset.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.2": {
     "name": "q3.2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not dummy_df is None, \"Have you used the correct variable to store the results?\"\n>>> assert sorted(list(dummy_df.columns)) == ['fit_time','score_time','test_score','train_score'], \"Your solution contains incorrect columns.\"\n>>> assert dummy_df.shape == (5,4), \"Are you carrying out 5-fold cross-validation and are you passing return_train_score=True?\"\n>>> assert sha1(str(round(dummy_df['test_score'].mean(),3)).encode('utf8')).hexdigest() == \"e04884a1f90ee71d58aa5550207e305cd0e18392\", \"The test scores seem wrong. Are you calling cross_validate correctly?\"\n>>> assert sha1(str(round(dummy_df['train_score'].mean(),3)).encode('utf8')).hexdigest() == \"e04884a1f90ee71d58aa5550207e305cd0e18392\", \"The train scores seem wrong. Are you calling cross_validate correctly?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.1": {
     "name": "q4.1",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not ordinal_transformer is None, \"Are you using the correct variable name?\"\n>>> assert type(ordinal_transformer.get_params()['categories'][0]) is list, \"Are you passing education levels as a list of lists?\"\n>>> assert ordinal_transformer.get_params()['dtype'] == int, \"Please set the dtype to int\"\n>>> assert (sha1(str(ordinal_transformer.get_params()['categories'][0]).encode('utf8')).hexdigest() == \"893a03d114b2af09b53247866c6eea54ebfd090f\") or (sha1(str(ordinal_transformer.get_params()['categories'][0]).encode('utf8')).hexdigest() == \"81059b8bebc9ddb03d61bf07cfd9b9b6b0da288e\"), \"Make sure you are passing categories sorted on levels of education. (Ascending or descending shouldn't matter.)\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.2": {
     "name": "q4.2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not binary_transformer is None, \"Are you using the correct variable name?\"\n>>> assert binary_transformer.get_params()['drop'] == 'if_binary', \"Are you passing `drop=if_binary`?\"\n>>> assert binary_transformer.get_params()['dtype'] == int, \"Please set the dtype to int\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.3": {
     "name": "q4.3",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not categorical_transformer is None, \"Are you using the correct variable name?\"\n>>> assert type(categorical_transformer) is Pipeline, \"Are you creating a scikit-learn Pipeline?\"\n>>> assert len(categorical_transformer.get_params()['steps']) == 2, \"Are you creating a pipeline with two steps?\"\n>>> assert categorical_transformer.get_params()['simpleimputer__strategy'] == 'constant', \"Are you passing strategy=constant in the SimpleImputer?\"\n>>> assert categorical_transformer.get_params()['simpleimputer__fill_value'] == 'missing', \"Are you passing fill_value='missing' in the SimpleImputer?\"\n>>> assert categorical_transformer.get_params()['onehotencoder__handle_unknown'] == 'ignore', \"Are you passing handle_unknown = 'ignore' argument to your OHE?\"\n>>> assert categorical_transformer.get_params()['onehotencoder__sparse'] == False, \"Are you creating a sparse matrix for OHE?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.4": {
     "name": "q4.4",
     "points": [
      5,
      1,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # task 1\n>>> assert not preprocessor is None, \"Are you using the correct variable name?\"\n>>> assert len(preprocessor.get_params()['transformers']) in range(4,6,1), \"Have you included all the transformers?\"\n>>> assert 'onehotencoder' in preprocessor.get_params().keys(), 'Either the categorical_transformer or binary_transformer is not included.'\n>>> assert 'standardscaler' in preprocessor.get_params().keys(), 'numeric_transformer is not included.'\n>>> assert 'ordinalencoder' in preprocessor.get_params().keys(), 'ordinal_transformer is not included.'\n>>> assert 'drop' in preprocessor.get_params().keys(), 'drop features step is not included.'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # task 2\n>>> assert not transformed_df is None, \"Are you using the correct variable name?\"\n>>> assert sha1(str(transformed_df.shape).encode('utf8')).hexdigest() == 'a0521f0cdbcd77cd213e7d1a3cfc13c1c7c92a6e', \"The shape of the transformed data is incorrect.\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert sha1(str(n_new_cols).encode('utf8')).hexdigest() == 'b7103ca278a75cad8f7d065acda0c2e80da0b7dc', \"The number of new columns (n_new_cols) is incorrect.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6.1": {
     "name": "q6.1",
     "points": [
      2,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # task 1\n>>> assert not final_pipeline is None, \"Are you using the correct variable name?\"\n>>> assert not test_score is None, \"Are you using the correct variable name?\"\n>>> assert len(final_pipeline.named_steps) == 2, \"The final pipeline needs to have two steps: one for the preprocessor and one for SVC.\"\n>>> assert final_pipeline.n_features_in_ == 14, \"Make sure to pass the original X_train to fit\"\n>>> assert final_pipeline.named_steps['svc'].get_params()['C'] == best_C, \"Are you using the best C value from the previous exercise?\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # task 2\n>>> assert not test_score is None, \"Are you using the correct variable name?\"\n>>> assert sha1(str(round(test_score,3)).encode('utf8')).hexdigest() == '1284d7d1d642ef8e51475cce5c1972ca6b8bd2b6', \"The test score seems off\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
